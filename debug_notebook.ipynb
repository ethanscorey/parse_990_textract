{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71227434",
   "metadata": {},
   "source": [
    "# Parsing Debug Notebook\n",
    "\n",
    "This somewhat messy notebook makes it easier to debug the parser, because we can just rerun the cells needed to set up the debugging process with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9e674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from parse_990_textract.bucket import open_df\n",
    "from parse_990_textract.filing import create_roadmap, extract_from_roadmap\n",
    "from parse_990_textract.models import BoundingBox, TableExtractor\n",
    "from parse_990_textract.parse import create_extractors, find_item, find_pages\n",
    "from parse_990_textract.postprocessing import clean_filing, clean_f_i, clean_f_ii, clean_f_iii, postprocess\n",
    "from parse_990_textract.setup import load_extractor_df\n",
    "from parse_990_textract.table import extract_table_data, find_table_pages, create_tablemap\n",
    "from parse_990_textract.utils import get_coordinate, get_regex, cluster_words, columnize, cluster_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af03fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = boto3.resource(\"s3\").Bucket(\"s3-ocr-990s-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4ebde",
   "metadata": {},
   "source": [
    "Setting `VALIDATE_TOP` to `True` will parse all 25 validation PDFs and compare non-Schedule F output to the validation data. If set to `False`, we run the test code for Schedule F instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45109bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATE_TOP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95aee98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if VALIDATE_TOP:\n",
    "    validation_data = pd.read_csv(\"validation_data.csv\", index_col=\"job_id\").fillna(\"\")\n",
    "    validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da6923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_df = load_extractor_df(\"parse_data/990_extractors.csv\")\n",
    "roadmap_df = load_extractor_df(\"parse_data/990_roadmap.csv\")\n",
    "schedule_f_tablemap_df = load_extractor_df(\"parse_data/schedule_f_table_roadmap.csv\")\n",
    "schedule_f_table_extractor_df = pd.read_csv(\"parse_data/schedule_f_table_extractors.csv\")\n",
    "schedule_f_row_extractor_df = pd.read_csv(\"parse_data/schedule_f_row_extractors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e87e1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PART_I_HEADER = r\"\\(a\\)\\s*Region|\\(d\\)\\s*Activities|\\(e\\)\\s*If activity|\\(f\\)Total expenditures\"\n",
    "PART_II_HEADER = r\"\\(b\\)\\s*IRS code|\\(c\\)\\s*Region|\\(d\\)\\s*Purpose|\\(f\\)\\s*Manner|\\(h\\)\\s*Description\"\n",
    "PART_III_HEADER = r\"\\(b\\)\\s*Region|\\(e\\)\\s*Manner of cash|\\(h\\)\\s*Method of va\"\n",
    "PART_I_TABLE_NAME = r\"Activities per Region\"\n",
    "PART_II_TABLE_NAME = r\"Grants to Organizations Outside the United States\"\n",
    "PART_III_TABLE_NAME = r\"Grants to Individuals Outside the United States\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de001784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filing_rows = []\n",
    "schedule_f_part_i_rows = []\n",
    "schedule_f_part_ii_rows = []\n",
    "schedule_f_part_iii_rows = []\n",
    "if VALIDATE_TOP:\n",
    "    values = validation_data.index.values\n",
    "else:\n",
    "    values = []\n",
    "\n",
    "for i, job_id in enumerate(values):\n",
    "    print(i)\n",
    "    print(job_id)\n",
    "    pdf_key = validation_data.at[job_id, \"pdf_key\"]\n",
    "    print(pdf_key)\n",
    "    \n",
    "    data = open_df(bucket, job_id)\n",
    "    lines = data.loc[data[\"BlockType\"] == \"LINE\"]\n",
    "    words = data.loc[data[\"BlockType\"] == \"WORD\"]\n",
    "    page_map = find_pages(lines)\n",
    "    roadmap = create_roadmap(\n",
    "        lines, roadmap_df, page_map\n",
    "    )\n",
    "    \n",
    "    row = extract_from_roadmap(\n",
    "        words, lines, roadmap, extractor_df, page_map\n",
    "    )\n",
    "    row = postprocess(row, job_id, pdf_key, clean_filing)\n",
    "    filing_rows.append(row)\n",
    "    \n",
    "    pages = lines.groupby(\"Page\")\n",
    "    \n",
    "    part_i_table = extract_table_data(\n",
    "        pages, lines, words, PART_I_HEADER, PART_I_TABLE_NAME, \n",
    "        schedule_f_tablemap_df, schedule_f_table_extractor_df, schedule_f_row_extractor_df,\n",
    "    )\n",
    "    part_i_table = postprocess(part_i_table, job_id, pdf_key, clean_f_i)\n",
    "    if part_i_table is not None:\n",
    "        schedule_f_part_i_rows.append(\n",
    "            part_i_table\n",
    "        )\n",
    "    part_ii_table = extract_table_data(\n",
    "        pages, lines, words, PART_II_HEADER, PART_II_TABLE_NAME, \n",
    "        schedule_f_tablemap_df, schedule_f_table_extractor_df, schedule_f_row_extractor_df,\n",
    "    )\n",
    "    part_ii_table = postprocess(part_ii_table, job_id, pdf_key, clean_f_ii)\n",
    "    if part_ii_table is not None:\n",
    "        schedule_f_part_ii_rows.append(\n",
    "            part_ii_table\n",
    "        )\n",
    "    part_iii_table = extract_table_data(\n",
    "        pages, lines, words, PART_III_HEADER, PART_III_TABLE_NAME, \n",
    "        schedule_f_tablemap_df, schedule_f_table_extractor_df, schedule_f_row_extractor_df,\n",
    "    )\n",
    "    part_iii_table = postprocess(part_iii_table, job_id, pdf_key, clean_f_iii)\n",
    "    if part_iii_table is not None:\n",
    "        schedule_f_part_iii_rows.append(\n",
    "            part_iii_table\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619b3527",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if VALIDATE_TOP:\n",
    "    output_data = pd.concat(filing_rows).reset_index(drop=True).set_index(\"job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9d25d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATE_TOP and output_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef1f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    x = str(x)\n",
    "    x = re.sub(r\"\\.0\\b\", \"\", x)\n",
    "    x = re.sub(\"\\D\", \"\", x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab8809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_output(to_validate, to_compare, col):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"extracted\": to_validate.loc[col].loc[\n",
    "                lambda series: series != to_compare.loc[col]\n",
    "            ],\n",
    "            \"expected\": to_compare.loc[col].loc[\n",
    "                lambda series: series != to_validate.loc[col]\n",
    "            ],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474b0edf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if VALIDATE_TOP:\n",
    "    to_compare = validation_data.set_index(\"pdf_key\").applymap(clean)\n",
    "    to_validate = output_data[validation_data.columns].set_index(\"pdf_key\").applymap(clean)\n",
    "\n",
    "    for col in to_validate.index:\n",
    "        validated = compare_output(to_validate, to_compare, col)\n",
    "        if validated.any().any():\n",
    "            print(col)\n",
    "            print(f\"{validated.shape[0]} mismatched items.\")\n",
    "            print(validated)\n",
    "            print(\"-\"*79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2ff2680",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATE_TOP and pd.concat(schedule_f_part_i_rows).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73bcc3e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATE_TOP and pd.concat(schedule_f_part_i_rows).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac2eefe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATE_TOP and pd.concat(schedule_f_part_ii_rows).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd8a386e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALIDATE_TOP and pd.concat(schedule_f_part_iii_rows).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75ce783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATE_TOP:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b56bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_test_df = open_df(bucket, \"4b6a0febec2c15d1432c6b4f450316397596e0f9ef8700bcd6846361eeac297a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e06ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lines = table_test_df.loc[\n",
    "    table_test_df[\"BlockType\"] == \"LINE\"\n",
    "]\n",
    "test_words = table_test_df.loc[\n",
    "    table_test_df[\"BlockType\"] == \"WORD\"\n",
    "]\n",
    "test_pages = test_lines.groupby(\"Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3919c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No match for address in 7/01 6/30 For the 2008 calendar year, or tax year beginning 2008, and ending Please use LIBERTY UNIVERSITY FOUNDATION IRS label or type or print 1971 UNIVERSITY BLVD See LYNCHBURG, VA 24502 specific Instruc- tions G\n",
      "No match for city in G\n",
      "No match for grants_us_govt_orgs_prog_service in expenses general\n",
      "No match for grants_us_govt_orgs_mgmt_and_general in expenses\n",
      "No match for grants_us_govt_orgs_fundraising in expenses\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>field_name</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>website</th>\n",
       "      <th>gross_receipts</th>\n",
       "      <th>year_formation</th>\n",
       "      <th>state_of_domicile</th>\n",
       "      <th>mission</th>\n",
       "      <th>...</th>\n",
       "      <th>activities_per_region_totals_number_of_offices</th>\n",
       "      <th>activities_per_region_totals_number_of_employees</th>\n",
       "      <th>activities_per_region_totals_total_expenditure</th>\n",
       "      <th>total_number_recipient_foreign_orgs_listed_as_charities</th>\n",
       "      <th>total_number_other_recipient_foreign_orgs_entities</th>\n",
       "      <th>job_id</th>\n",
       "      <th>pdf_key</th>\n",
       "      <th>ein</th>\n",
       "      <th>year</th>\n",
       "      <th>filing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIBERTY UNIVERSITY FOUNDATION</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>VA</td>\n",
       "      <td>24502</td>\n",
       "      <td>N/A</td>\n",
       "      <td>222685</td>\n",
       "      <td>2001</td>\n",
       "      <td>DC</td>\n",
       "      <td>Receive, administer and expend funds for Chani...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>foo_2_3_4_5</td>\n",
       "      <td>bar_2_3_4_5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "field_name                           name address city state    zip website  \\\n",
       "0           LIBERTY UNIVERSITY FOUNDATION                 VA  24502     N/A   \n",
       "\n",
       "field_name gross_receipts year_formation state_of_domicile  \\\n",
       "0                  222685           2001                DC   \n",
       "\n",
       "field_name                                            mission  ...  \\\n",
       "0           Receive, administer and expend funds for Chani...  ...   \n",
       "\n",
       "field_name activities_per_region_totals_number_of_offices  \\\n",
       "0                                                           \n",
       "\n",
       "field_name activities_per_region_totals_number_of_employees  \\\n",
       "0                                                             \n",
       "\n",
       "field_name activities_per_region_totals_total_expenditure  \\\n",
       "0                                                           \n",
       "\n",
       "field_name total_number_recipient_foreign_orgs_listed_as_charities  \\\n",
       "0                                                                    \n",
       "\n",
       "field_name total_number_other_recipient_foreign_orgs_entities       job_id  \\\n",
       "0                                                              foo_2_3_4_5   \n",
       "\n",
       "field_name      pdf_key ein year filing_id  \n",
       "0           bar_2_3_4_5   2    4       2_4  \n",
       "\n",
       "[1 rows x 186 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_map = find_pages(test_lines)\n",
    "roadmap = create_roadmap(\n",
    "    test_lines, roadmap_df, page_map\n",
    ")\n",
    "\n",
    "row = extract_from_roadmap(\n",
    "    test_words, test_lines, roadmap, extractor_df, page_map\n",
    ")\n",
    "row = postprocess(row, \"foo_2_3_4_5\", \"bar_2_3_4_5\", clean_filing)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b135a6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Page, dtype: int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lines.loc[\n",
    "    (test_lines[\"Page\"] == page_map[\"Page 1\"])\n",
    "    & test_lines[\"Text\"].str.contains(\n",
    "        \"Net rental income|Direct public|2007 calendar\"\n",
    "    ),\n",
    "    \"Page\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83217d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = PART_I_HEADER\n",
    "NAME = PART_I_TABLE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45fd9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_pages = find_table_pages(\n",
    "    test_pages[\"Text\"].agg(lambda words: \" \".join(words)),\n",
    "    HEADER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "261934b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Page, dtype: int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69522c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INDEX = 1\n",
    "TEST_PAGE = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb9879b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablemaps = pd.DataFrame(\n",
    "    {\n",
    "        \"page\": table_pages,\n",
    "        \"tablemap\": table_pages.map(\n",
    "            lambda page: create_tablemap(\n",
    "                test_lines, schedule_f_tablemap_df, page, NAME\n",
    "            ).dropna()\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04aa2533",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtablemaps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtablemap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEST_INDEX\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parse_990_textract/lib/python3.9/site-packages/pandas/core/indexing.py:1074\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1071\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1073\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/parse_990_textract/lib/python3.9/site-packages/pandas/core/indexing.py:1626\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1626\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/parse_990_textract/lib/python3.9/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "tablemaps[\"tablemap\"].iloc[TEST_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b01610",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_extractors = schedule_f_row_extractor_df.loc[\n",
    "    schedule_f_row_extractor_df[\"table\"] == NAME\n",
    "]\n",
    "table_data = schedule_f_table_extractor_df.loc[\n",
    "    schedule_f_table_extractor_df[\"table\"] == NAME\n",
    "].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = tablemaps.assign(\n",
    "    extractor=tablemaps[\"tablemap\"].map(\n",
    "        lambda tablemap: TableExtractor(\n",
    "            header_top_label=table_data[\"header_top\"],\n",
    "            top_label=table_data[\"table_top\"],\n",
    "            bottom_label=table_data[\"table_bottom\"],\n",
    "            tablemap=tablemap,\n",
    "            fields=row_extractors[\"field\"],\n",
    "            field_labels=row_extractors[\"col_left\"]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8599ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_extractor = rows[\"extractor\"].iloc[TEST_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebafcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_extractor.get_col_spans(test_words, TEST_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07808559",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extractor.field_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extractor.header_top_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd690e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extracted = test_extractor.extract_rows(test_words, TEST_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21698813",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e48e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table_words = test_extractor.get_table_words(test_words, TEST_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2c462",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_clusters = cluster_words(table_words, table_words[\"Height\"].min(), \"Midpoint_Y\")\n",
    "[\" \".join(word.sort_values(by=\"Left\")[\"Text\"].values) for word in word_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10201c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd27b17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def columnize(word_cluster, col_spans):\n",
    "    return col_spans.map(\n",
    "        lambda span: word_cluster.loc[\n",
    "            (word_cluster[\"Right\"].between(*span, inclusive=\"right\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cluster_coords(cluster):\n",
    "    cluster_coords = {\n",
    "        \"Left\": cluster[\"Left\"].min(),\n",
    "        \"Right\": cluster[\"Right\"].max(),\n",
    "        \"Height\": cluster[\"Height\"].max(),\n",
    "        \"Midpoint_X\": cluster[\"Midpoint_X\"].median(),\n",
    "        \"Midpoint_Y\": cluster[\"Midpoint_Y\"].median(),\n",
    "        \"Top\": cluster[\"Top\"].min(),\n",
    "        \"Bottom\": cluster[\"Bottom\"].min(),\n",
    "    }\n",
    "    cluster_coords[\"Width\"] = cluster_coords[\"Right\"] - cluster_coords[\"Left\"]\n",
    "    return cluster_coords\n",
    "\n",
    "\n",
    "def combine_row(row):\n",
    "    return pd.Series([\n",
    "        line.map(\n",
    "            lambda x: x.sort_values(\n",
    "                by=\"Left\"\n",
    "            ).reset_index(drop=True)[\"Text\"].fillna(\"\")\n",
    "        ).agg(\n",
    "            lambda x: \" \".join(x.values)\n",
    "        ) + \" \"\n",
    "        for line in row\n",
    "    ]).sum().str.strip()\n",
    "\n",
    "col_spans = test_extractor.get_col_spans(test_words, TEST_PAGE)\n",
    "\n",
    "col_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b2b6b",
   "metadata": {},
   "source": [
    "Row break scenarios:\n",
    "1. Previous cluster and current cluster both have entries in the same numeric column\n",
    "2. Alignment is TOP and current cluster has non-empty columns that are empty in previous row\n",
    "3. Alignment is BOTTOM and current cluster has empty columns that are non-empty in previous row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93ee01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_tol = table_words[\"Height\"].median()\n",
    "columnized = columnize(word_clusters[0], col_spans)\n",
    "columnized.index = test_extractor.fields\n",
    "last_col_coords = pd.DataFrame.from_records(\n",
    "    columnized.map(\n",
    "        get_cluster_coords\n",
    "    )\n",
    ")\n",
    "rows = []\n",
    "if NAME == PART_I_TABLE_NAME:\n",
    "    numeric_cols = (1,2,5)\n",
    "elif NAME == PART_II_TABLE_NAME:\n",
    "    numeric_cols = (4,6)\n",
    "elif NAME == PART_III_TABLE_NAME:\n",
    "    numeric_cols = (2,3,5)\n",
    "current_row = [columnized]\n",
    "top_ws = (\n",
    "    last_col_coords[\"Top\"].min()\n",
    "    - test_extractor.get_table_top(test_words, TEST_PAGE)\n",
    ")\n",
    "print(f\"Y tolerance: {y_tol}\")\n",
    "print(f\"Top whitespace: {top_ws}\")\n",
    "if top_ws > y_tol * 4:\n",
    "    alignment = \"BOTTOM\"\n",
    "else:\n",
    "    alignment = \"UNKNOWN\"\n",
    "print(f\"Alignment: {alignment}\")\n",
    "print(\"First cluster:\")\n",
    "print(\" \".join(word_clusters[0].sort_values(by=\"Left\")[\"Text\"].values))\n",
    "for count, cluster in enumerate(word_clusters[1:]):\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Alignment: {alignment}\")\n",
    "    print(\"Cluster:\", \" \".join(cluster.sort_values(by=\"Left\")[\"Text\"].values))\n",
    "    columnized = columnize(cluster, col_spans)\n",
    "    columnized.index = test_extractor.fields\n",
    "    col_coords = pd.DataFrame.from_records(columnized.map(get_cluster_coords))\n",
    "    nonempty = col_coords.dropna().index.to_series()\n",
    "    last_nonempty = last_col_coords.dropna().index.to_series()\n",
    "    print(\"Nonempty\\n\", nonempty)\n",
    "    print(\"Last nonempty\\n\", last_nonempty)\n",
    "    # more_cols true if current row has non-empty cells that\n",
    "    # are empty in the preceding row\n",
    "    more_cols = (~nonempty.isin(last_nonempty)).any()\n",
    "    print(\"More cols:\", more_cols)\n",
    "    # less_cols true if last row has non-empty cells that\n",
    "    # are empty in current row\n",
    "    less_cols = (~last_nonempty.isin(nonempty)).any()\n",
    "    print(\"Less cols:\", less_cols)\n",
    "    # both_numeric true if both rows have entries in numeric cols\n",
    "    both_numeric = (\n",
    "        nonempty.isin(numeric_cols) \n",
    "        & last_nonempty.isin(numeric_cols)\n",
    "    ).any()\n",
    "    y_delta = (\n",
    "        col_coords[\"Top\"].min()\n",
    "        - last_col_coords[\"Bottom\"].max()\n",
    "    )\n",
    "    print(\"Y delta\", y_delta)\n",
    "    print(\"Both numeric:\", both_numeric)\n",
    "    if (\n",
    "        both_numeric\n",
    "        or (more_cols and (alignment== \"TOP\"))\n",
    "        or (less_cols and (alignment == \"BOTTOM\"))\n",
    "        or (y_delta > y_tol)\n",
    "    ):\n",
    "        combined_row = combine_row(current_row)\n",
    "        print(combined_row)\n",
    "        rows.append(combined_row)\n",
    "        current_row = [columnized]\n",
    "    elif less_cols and (alignment == \"UNKNOWN\"):\n",
    "        alignment = \"TOP\"\n",
    "        current_row.append(columnized)\n",
    "    elif more_cols and (alignment == \"UNKNOWN\"):\n",
    "        alignment = \"BOTTOM\"\n",
    "        current_row.append(columnized)\n",
    "    else:\n",
    "        current_row.append(columnized)\n",
    "    last_col_coords = col_coords            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ea284",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_words = test_extractor.get_header_words(test_words, TEST_PAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cca982",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "header_words.sort_values(by=\"Left\")[[\"Text\", \"Left\", \"Right\", \"Midpoint_Y\"]].tail(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
